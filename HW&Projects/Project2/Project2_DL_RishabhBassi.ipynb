{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2893,"status":"ok","timestamp":1700959079880,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"gSZk3AFWTZTp"},"outputs":[],"source":["import tensorflow as tf\n","import pickle\n","import string\n","import re\n","import numpy as np\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense, Embedding\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.optimizers import Adam"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16055,"status":"ok","timestamp":1700959097475,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"BL_5vm1fTk5H","outputId":"a30897d5-c504-4328-acc4-494dab98620b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RxMCD9sGTjnY","executionInfo":{"status":"ok","timestamp":1700959099726,"user_tz":360,"elapsed":1296,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"}}},"outputs":[],"source":["# Load the input and output data\n","train_input_texts = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/DL/Project2/Train_input', 'rb'))\n","train_output_texts = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/DL/Project2/Train_output', 'rb'))"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1700959102050,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"-cux-c3OTmYJ","outputId":"1a658f38-1d54-4242-ced1-0825cf6cc2c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["64\n","99\n"]}],"source":["#Checking length of input and output max\n","input_lengthMax = max([len(txt) for txt in train_input_texts])\n","output_lengthMax = max([len(txt) for txt in train_output_texts])\n","print(input_lengthMax)\n","print(output_lengthMax)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"0xlMQR1mTpH6","executionInfo":{"status":"ok","timestamp":1700959103606,"user_tz":360,"elapsed":142,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"}}},"outputs":[],"source":["#Like in the github for Ch11, added both input and output in one list seperated by comma and added a start and end token for output\n","language = []\n","for line_ip, line_op in zip(train_input_texts, train_output_texts):\n","    output_lng = \"[start] \" + line_op + \" [end]\"\n","    language.append((line_ip, output_lng))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1700959104944,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"McMkSlF2Tqm-","outputId":"cd845867-bd6f-4e9f-cf78-f588e6d7b496"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('a d a d b d a e b d a g c g a g c f c f ',\n"," '[start] b d b d a e e a d d f c g c f c f a g i j a g h k a d g l  [end]')"]},"metadata":{},"execution_count":6}],"source":["language[1]"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uBjXYlpRTsEv","executionInfo":{"status":"ok","timestamp":1700959129571,"user_tz":360,"elapsed":306,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"}}},"outputs":[],"source":["#split into training and validation set\n","import random\n","random.shuffle(language)\n","val_set_num = int(0.15 * len(language))\n","train_set_num = len(language) - val_set_num - val_set_num//6\n","train_set = language[:train_set_num]\n","val_set = language[train_set_num:train_set_num + val_set_num]\n","test_set = language[train_set_num+val_set_num:]"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1700959130579,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"ieBdJgx-Tust","outputId":"04b62e51-e33e-49e1-8d1a-2de636819d22"},"outputs":[{"output_type":"stream","name":"stdout","text":["16800\n","92400\n","2800\n"]}],"source":["print(val_set_num)\n","print(train_set_num)\n","print(len(test_set))"]},{"cell_type":"markdown","metadata":{"id":"fALGvG0yT8WD"},"source":["Vectorizing the input language and output language text pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zj8IcaarT49N"},"outputs":[],"source":["strip_chars = string.punctuation + \"Â¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","vocab_size = 40 #total unique words being 37 approx 40\n","sequence_length = 300 #max length of output being about 211 so approx kept it as 300\n","\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_inp_lang = [pair[0] for pair in train_set]\n","train_otp_lang = [pair[1] for pair in train_set]\n","source_vectorization.adapt(train_inp_lang)\n","target_vectorization.adapt(train_otp_lang)"]},{"cell_type":"markdown","metadata":{"id":"3uOg-pHHT-fo"},"source":["Preparing datasets for the translation task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqnV8j0aUALd"},"outputs":[],"source":["batch_size = 64\n","\n","def format_dataset(inp, otp):\n","    inp = source_vectorization(inp)\n","    otp = target_vectorization(otp)\n","    return ({\n","        \"language_1\": inp,\n","        \"language_2\": otp[:, :-1],\n","    }, otp[:, 1:])\n","\n","def make_dataset(pairs):\n","    inp_lang, otp_lang = zip(*pairs)\n","    inp_lang = list(inp_lang)\n","    otp_lang = list(otp_lang)\n","    dataset = tf.data.Dataset.from_tensor_slices((inp_lang, otp_lang))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_dataset = make_dataset(train_set)\n","val_dataset = make_dataset(val_set)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1501,"status":"ok","timestamp":1700443630945,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"Qw8ZkM-lUFB8","outputId":"5368bad5-1a9d-4452-a8af-ce62d2da24cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["inputs['language_1'].shape: (64, 300)\n","inputs['language_2'].shape: (64, 300)\n","targets.shape: (64, 300)\n"]}],"source":["for inputs, targets in train_dataset.take(1):\n","    print(f\"inputs['language_1'].shape: {inputs['language_1'].shape}\")\n","    print(f\"inputs['language_2'].shape: {inputs['language_2'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"19oeSqZcUIo2"},"source":["Transformer encoder implemented as a subclassed Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ee2obVRwUIXq"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        #MultiHeadAttention layer that implements the multi-head attention mechanism\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        #dense projection layer, applies two dense layers with ReLU activation and then a final dense layer with no activation function\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        #Two LayerNormalization layers that normalize the input vectors and the output vectors, respectively\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    #dictionary containing the configuration of the layer\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim, #dimension of the input vector, and also the output vector\n","            \"num_heads\": self.num_heads, #number of attention heads to use in the Multi-Head Attention mechanism\n","            \"dense_dim\": self.dense_dim, #dimension of the intermediate vector produced by the dense projection layer\n","        })\n","        return configv"]},{"cell_type":"markdown","metadata":{"id":"m5u_owEtUNKs"},"source":["Transformer Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eTfIG6e-UO7d"},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim #dimensionality of the input embedding and output embeddings\n","        self.dense_dim = dense_dim #dimensionality of the hidden layer in the feedforward network inside the decoder\n","        self.num_heads = num_heads #number of attention heads to use in the multi-head attention layers\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","#implements the forward pass of the Decoder layer\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        else:\n","            padding_mask = mask\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        return self.layernorm_3(attention_output_2 + proj_output)"]},{"cell_type":"markdown","metadata":{"id":"fRaL-hzsUSgW"},"source":["Using positional encoding to re-inject order information.\n","Implementing positional embedding as a subclassed layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0MsOISrUXId"},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"_m4yls0_UcjN"},"source":["Putting it all together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHcQ1Rt2UeGj"},"outputs":[],"source":["embed_dim = 256\n","dense_dim = 2048\n","num_heads = 8\n","#model takes two inputs: encoder_inputs and decoder_inputs\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"language_1\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs) # adds positional encoding to the input embeddings\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) #output of the PositionalEmbedding layer is passed through a TransformerEncoder\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"language_2\")\n","#decoder_inputs are also passed through a PositionalEmbedding layer, and then through a TransformerDecoder layer\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","x = layers.Dropout(0.5)(x) #Dropout layer is applied to the output of the TransformerDecoder layer to prevent overfitting\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) #Dense layer with a softmax activation is applied to produce the output probability distribution over the vocabulary for each position in the output sequence\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298,"status":"ok","timestamp":1700443645732,"user":{"displayName":"Rishabh Bassi","userId":"13997062773008787061"},"user_tz":360},"id":"9DlwoTKoUglq","outputId":"bfd42d7d-36ec-46ea-a7d6-88afc666e180"},"outputs":[{"data":{"text/plain":["<bound method Model.summary of <keras.src.engine.functional.Functional object at 0x7aa0cdff2680>>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["transformer.summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMK7SMHYUiHU"},"outputs":[],"source":["#To save the model\n","callbacks =[keras.callbacks.ModelCheckpoint(\"final_transformer120.keras\",save_best_only=True)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ckN8vG1XUltx","outputId":"82343c5b-4275-492d-f5b7-a1ba1752411e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/120\n","   2/1400 [..............................] - ETA: 16:46:26 - loss: 4.6123 - accuracy: 0.1332"]}],"source":["transformer.compile(\n","    optimizer=Adam(),\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","transformer.fit(train_dataset, epochs=120, validation_data=val_dataset,callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"VrZOk8mmUqFw"},"source":["Translating few lines with Transformer model to check output syntax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r20mOOuvUr8L"},"outputs":[],"source":["import numpy as np\n","output_vocab = target_vectorization.get_vocabulary()\n","output_index_lookup = dict(zip(range(len(output_vocab)), output_vocab))\n","max_decoded_sentence_length = 100\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = output_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    decoded_sentence = decoded_sentence.replace('[start] ','').replace(' [end]','')\n","    return decoded_sentence\n","\n","test_input_train_texts = [pair[0] for pair in train_set]\n","for _ in range(5):\n","    input_sentence = random.choice(test_input_train_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"code","source":["from tensorflow.keras import models\n","import pickle\n","import tensorflow as tf\n","\n","model = models.load_model(\"/content/drive/MyDrive/Colab Notebooks/DL/Project2/Rishabh_Bassi_532008692_Project2_Model.h5\")\n","test_output = test_set\n","\n","# Include your data preprocessing code if applicable\n","# <your data preprocessing code>\n","# Include your data preprocessing code if applicable\n","\n","test_loss, test_acc = model.evaluate(test_output)\n","your_score = round(test_acc*1000) / 10\n","print(f\"Your Score: {your_score}\")"],"metadata":{"id":"zG6UnLvaFCk-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","output_vocab = target_vectorization.get_vocabulary()\n","output_index_lookup = dict(zip(range(len(output_vocab)), output_vocab))\n","max_decoded_sentence_length = sequence_length\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = output_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    decoded_sentence = decoded_sentence.replace('[start] ','').replace(' [end]','')\n","    return decoded_sentence\n","\n","\n","\n","test_input_train_texts = [pair[0] for pair in test_set]\n","test_output_train_texts = [pair[1] for pair in test_set]\n","total_sentences = len(test_set)\n","count=0\n","i=1\n","for pair in test_set:\n","    truesq = pair[1].replace('[start] ','').replace(' [end]','')\n","    outputseq=decode_sequence(pair[0]).strip()\n","    if(outputseq==truesq.strip()):\n","        count+=1\n","    if(i%200==0):\n","        print(\"Accuracy and I\",i,count/i);\n","    i+=1\n","#     print(\"True Output \",truesq.strip())\n","#     print(\"Predicted \",outputseq)\n","\n","\n","test_acc = count / total_sentences\n","your_score = round(test_acc * 1000) / 10\n","print(f\"Your Test Accuracy: {test_acc:.4f}\")\n","print(f\"Your Score: {your_score}\")\n","\n","# for _ in range(5):\n","#     input_sentence = random.choice(test_input_train_texts)\n","#     print(\"-\")\n","#     print(input_sentence)\n","#     print(decode_sequence(input_sentence))\n","\n","\n","\n","total_sentences = len(test_output)\n","total_correct_sentences = sum(pred_sentence.strip() == true_sentence.strip() for pred_sentence, true_sentence in zip(test_prediction, test_output))\n","\n","test_acc = total_correct_sentences / total_sentences\n","your_score = round(test_acc * 1000) / 10\n","print(f\"Your Test Accuracy: {test_acc:.4f}\")\n","print(f\"Your Score: {your_score}\")"],"metadata":{"id":"MynHbDgBES3o"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP05iktDdXXbdmZQzrSyl4o"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}