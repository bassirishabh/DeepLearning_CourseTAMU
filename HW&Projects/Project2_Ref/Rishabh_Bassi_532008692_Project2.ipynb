{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Deep Learning :  Machine Translation for two \"artificial\" languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input and output data\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Language by combining input and output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "szxFmVtPwWq3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "train_input_texts = pickle.load(open('Train_input', 'rb'))\n",
    "train_output_texts = pickle.load(open('Train_output', 'rb'))\n",
    "\n",
    "# Getting the max length of input and output\n",
    "input_lengthMax = max([len(txt) for txt in train_input_texts])\n",
    "output_lengthMax = max([len(txt) for txt in train_output_texts])\n",
    "\n",
    "print(input_lengthMax)\n",
    "print(output_lengthMax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training and Testing Purposes, I made a split in dataset where I took a small proportion of datasamples for my testing and verfication, below is train set I used for training and testing. After this, 80-20 split is done on this train set for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language = pickle.load(open('train_set', 'rb')) # Test Set of 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "v5xGstbcwWln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "112000\n",
      "('a d a d b d a e b d a g c g a g c f c f ', '[start] b d b d a e e a d d f c g c f c f a g i j a g h k a d g l  [end]')\n"
     ]
    }
   ],
   "source": [
    "language = []\n",
    "max_line_op_length = 0\n",
    "for line_ip, line_op in zip(train_input_texts, train_output_texts):\n",
    "    output_lng = \"[start] \" + line_op + \" [end]\" \n",
    "    language.append((line_ip, output_lng))\n",
    "    \n",
    "    current_line_op_length = len(output_lng)\n",
    "    \n",
    "    if current_line_op_length > max_line_op_length:\n",
    "        max_line_op_length = current_line_op_length\n",
    "\n",
    "print(max_line_op_length)\n",
    "print(len(language))\n",
    "print(language[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V_Y7-wSxwzeX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22400\n",
      "89600\n"
     ]
    }
   ],
   "source": [
    "# split of data into train and validation\n",
    "# random.shuffle(language)\n",
    "val_set_num = int(0.2 * len(language))\n",
    "train_set_num = len(language) - val_set_num\n",
    "train_set = language[:train_set_num]\n",
    "val_set = language[train_set_num:train_set_num + val_set_num]\n",
    "print(val_set_num)\n",
    "print(train_set_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_xfvs0WKgSK"
   },
   "source": [
    "### Doing the Source and Target Vectorization of the input language and output language\n",
    "#### Vocab Size and Sequence Length are chosen by analyzing the tokens and language and by experimenting with the different values in that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lKGeciJd92Cj"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    strip_chars = string.punctuation\n",
    "    strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "    strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 22 # Analyzing the language \n",
    "sequence_length = 130 # max length of output being about 113 so approx kept it as 150\n",
    "\n",
    "source_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_inp_lang = [pair[0] for pair in train_set]\n",
    "train_otp_lang = [pair[1] for pair in train_set]\n",
    "source_vectorization.adapt(train_inp_lang)\n",
    "target_vectorization.adapt(train_otp_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eduDjckRLEHf"
   },
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LDyOn0Xt91-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['language_1'].shape: (64, 130)\n",
      "inputs['language_2'].shape: (64, 130)\n",
      "targets.shape: (64, 130)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(inp, otp):\n",
    "    inp = source_vectorization(inp)\n",
    "    otp = target_vectorization(otp)\n",
    "    return ({\n",
    "        \"language_1\": inp,\n",
    "        \"language_2\": otp[:, :-1],\n",
    "    }, otp[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    inp_lang, otp_lang = zip(*pairs)\n",
    "    inp_lang = list(inp_lang)\n",
    "    otp_lang = list(otp_lang)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inp_lang, otp_lang))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_dataset = make_dataset(train_set)\n",
    "val_dataset = make_dataset(val_set)\n",
    "\n",
    "for inputs, targets in train_dataset.take(1):\n",
    "    print(f\"inputs['language_1'].shape: {inputs['language_1'].shape}\")\n",
    "    print(f\"inputs['language_2'].shape: {inputs['language_2'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN24SXRTLt9b"
   },
   "source": [
    "### Transformer Encoder : Implementing Layer as a subclass using dense layers and relu activation function. \n",
    "#### Different experiments were done with activation function, MultiHead Attention Layer and dimesions but this is the code which gave the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LErT7lpsD8BU"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        # Implemention of the multi-head attention mechanism\n",
    "        self.attention = layers.MultiHeadAttention( \n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Two dense layers : ReLU activation + final dense layer with no activation function\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # Two LayerNormalization layers\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim, # input,output vector dimensions\n",
    "            \"num_heads\": self.num_heads, # no of heads for use in the multi-head attention layers\n",
    "            \"dense_dim\": self.dense_dim, # intermediate vector dimensions after the dense projection layer\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfHkEo-hMM2G"
   },
   "source": [
    "### Transformer Decoder : Implementing Layer as a subclass using dense layers and relu activation function. \n",
    "#### Different experiments were done with activation function, Normalization layers and dimesions but this is the code which gave the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RH745TGUD7-i"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim # dimensionality of embedding\n",
    "        self.dense_dim = dense_dim # hidden layer dimensionality in the feedforward network inside the decoder\n",
    "        self.num_heads = num_heads # no of heads for use in the multi-head attention layers\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = mask\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJkwxX5nL-iG"
   },
   "source": [
    "### Positional Embeddings to re-inject order information. Implementing positional embedding as a subclassed layer.\n",
    "#### Tried various optimizations such as sinusodial positional and absolute embeddings but the best one was Learned Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dqO9SJFQD770"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim, \n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim, \n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3208w16SMb_B"
   },
   "source": [
    "### Summing up the Model by calling the layers\n",
    "#### 1. Various experiments were done with the dimension, num heads  but the best one is kept. \n",
    "#### 2. Different versions of optimizer were also tried and experiments with learning rate such as Decay Learning Rate were also tried.\n",
    "#### 3. Experimented with the dropout layers and percentage to do regularization.\n",
    "#### 4. Early Stopping was also tried with different values of patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Z1-ZrT-EDpj"
   },
   "outputs": [],
   "source": [
    "# embed_dim = 512 \n",
    "# dense_dim = 4096\n",
    "# num_heads = 16\n",
    "embed_dim = 256 \n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "# We have two inputs which are passed to model : encoder and decoder\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"language_1\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs) # using positional embeddings to input\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # Encoder takes output of positional embeddings\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"language_2\") # same flow as above for decode rlayer\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x) #Dropout layer is applied to the output of the TransformerDecoder layer to prevent overfitting\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) # A dense layer with softmax activation is utilized to generate the probability distribution across the vocabulary for every position in the output sequence.\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ILarOVTVEDmt",
    "outputId": "0fc18e58-3fcf-4ef6-e0b9-38daa1e8c921"
   },
   "outputs": [],
   "source": [
    "transformer.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIyZN4tPGRT6"
   },
   "outputs": [],
   "source": [
    "# Model Saving\n",
    "callbacks =[keras.callbacks.ModelCheckpoint(\"Rishabh_Bassi_532008692_Project2_Model.h5\",save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# initial_learning_rate = 0.001\n",
    "# end_learning_rate = 0.0001\n",
    "# epochs = 120\n",
    "# decay_steps = len(train_dataset) * epochs\n",
    "# lr_schedule = PolynomialDecay(\n",
    "#     initial_learning_rate, decay_steps, end_learning_rate, power=0.5\n",
    "# )\n",
    "\n",
    "# optimizer = Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1npjXNlD915b",
    "outputId": "7219f792-b497-4350-8dec-14de1ee3efac"
   },
   "outputs": [],
   "source": [
    "epochs = 45\n",
    "transformer.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "transformer.fit(train_dataset, epochs=epochs, validation_data=val_dataset,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Testing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNbTWSt8Mll7"
   },
   "source": [
    "### Code for Generating Predictions and Local Verification of Model Generated by Separating some percentage of Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two cells which you have to run for Generating the predictions, In the first cell you have to replace the test input in second line and load into test_Input.\n",
    "Second Cell Execution will require you to run the above cells for classes where Transformer encoder, decoder etc. are implemented. So before running this cell, kindly run the above cells from starting till code cell 9(Positional embeddings) except the model training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = pickle.load(open('test_set', 'rb'))\n",
    "# test_input = [sentence[0] for sentence in test_set] \n",
    "\n",
    "# Just uncomment the below line and comment the above two lines if you want to generate the predictions for your test input. Here the actual test input will be loaded\n",
    "test_input = pickle.load(open('Test_input_1000', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionGeneration(model,input_lines):\n",
    "    output_vocab = target_vectorization.get_vocabulary()\n",
    "    lookup_index = dict(zip(range(len(output_vocab)), output_vocab))\n",
    "    tokenized_input_line = source_vectorization([input_lines])\n",
    "    translated_line = \"[start]\"\n",
    "    for i in range(sequence_length):\n",
    "        tokenized_output_line = target_vectorization(\n",
    "            [translated_line])[:, :-1]\n",
    "        predict = model(\n",
    "            [tokenized_input_line, tokenized_output_line])\n",
    "        token_idx = np.argmax(predict[0, i, :])\n",
    "        token = lookup_index[token_idx]\n",
    "        translated_line += \" \" + token\n",
    "        if token == \"[end]\":\n",
    "            break\n",
    "    return translated_line\n",
    "    \n",
    "def modelTesting(model,test_data):\n",
    "\n",
    "    predictions = []  \n",
    "    \n",
    "    for data in test_data:\n",
    "        predict = predictionGeneration(model,data)\n",
    "        predict = predict.replace('[start] ', '').replace(' [end]', '')\n",
    "        predictions.append(predict)\n",
    "\n",
    "    return predictions\n",
    "    \n",
    "model = keras.models.load_model(\"Rishabh_Bassi_532008692_Project2_Model.h5\",\n",
    "    custom_objects={\"TransformerDecoder\": TransformerDecoder,\n",
    "                    \"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "                    \n",
    "                    \n",
    "prediction_set = modelTesting(model,test_input)\n",
    "\n",
    "pickle.dump(prediction_set, open('Rishabh_Bassi_532008692_Project2_Prediction','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the cell which you can execute to get the accuracy of model given that prediction file is provided. Before Running this cell, Prediction file needs to be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Testing code placeholder \n",
    "import pickle\n",
    "\n",
    "# Loading datasets\n",
    "test_output = pickle.load(open(\"test_output\", \"rb\")) # your filename needs to be here to compare predictions\n",
    "test_prediction = pickle.load(open(\"Rishabh_Bassi_532008692_Project2_Prediction\", \"rb\"))\n",
    "\n",
    "total_sentences = len(test_output)\n",
    "total_correct_sentences = sum(pred_sentence.strip() == true_sentence.strip() for pred_sentence, true_sentence in zip(test_prediction, test_output))\n",
    "\n",
    "test_acc = total_correct_sentences / total_sentences\n",
    "your_score = round(test_acc * 1000) / 10\n",
    "print(f\"Your Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Your Score: {your_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference for Local Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of True Output vs Size of Preds: 500 500\n",
      "Your Test Accuracy: 0.9800\n",
      "Your Score: 98.0\n"
     ]
    }
   ],
   "source": [
    "#This code is for local testing\n",
    "test_output = [sentence[1] for sentence in test_set]\n",
    "print(\"Size of True Output vs Size of Preds:\",len(test_output),len(prediction_set))\n",
    "total_sentences = len(test_input)\n",
    "\n",
    "correct_count = 0\n",
    "for i in range(len(prediction_set)):\n",
    "    trueout = test_output[i].replace('[start] ','').replace(' [end]','')\n",
    "    \n",
    "    if(trueout.strip()==prediction_set[i].strip()):\n",
    "        correct_count+=1\n",
    "\n",
    "test_acc = correct_count / total_sentences\n",
    "your_score = round(test_acc * 1000) / 10\n",
    "print(f\"Your Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Your Score: {your_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
